{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "j1dhp5wowV9Z"
      ],
      "authorship_tag": "ABX9TyOZjVTh0/n/m7iGGnF/37BG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sufyanAshraf/personal/blob/main/ramal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GsUuFMFUUpp_"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary to store ramal values\n",
        "ramal_dict = {}\n",
        "\n",
        "# Define the ramal values for each number from 1 to 16 where\n",
        "# 0 represents a dot and 1 represents a line\n",
        "ramal_values = {\n",
        "    1:  [0, 1, 1, 1],\n",
        "    2:  [1, 0, 1, 1],\n",
        "    3:  [0, 0, 1, 1],\n",
        "    4:  [1, 1, 0, 1],\n",
        "    5:  [0, 1, 0, 1],\n",
        "    6:  [1, 0, 0, 1],\n",
        "    7:  [0, 0, 0, 1],\n",
        "    8:  [1, 1, 1, 0],\n",
        "    9:  [0, 1, 1, 0],\n",
        "    10: [1, 0, 1, 0],\n",
        "    11: [0, 0, 1, 0],\n",
        "    12: [1, 1, 0, 0],\n",
        "    13: [0, 1, 0, 0],\n",
        "    14: [1, 0, 0, 0],\n",
        "    15: [0, 0, 0, 0],\n",
        "    16: [1, 1, 1, 1]\n",
        "}\n",
        "\n",
        "# Function to create dictionary with 0, 1, 2, 3 as keys and complete value (cvalue)\n",
        "def calculate_ramal():\n",
        "    for num, ramal in ramal_values.items():\n",
        "        # Create a string representation of the ramal value (cvalue)\n",
        "        cvalue = ''.join(str(bit) for bit in ramal)\n",
        "        # Store each bit in the dictionary with 0, 1, 2, and 3\n",
        "        ramal_dict[num] = {i: bit for i, bit in enumerate(ramal)}\n",
        "        ramal_dict[num]['cvalue'] = cvalue\n",
        "\n",
        "\n",
        "def find_number_with_bit_list(bit_list): #it returns key of match\n",
        "    # Find the matching ramal value\n",
        "    for key, value in ramal_values.items():\n",
        "        if value == bit_list:\n",
        "            return key\n",
        "    return None\n",
        "\n",
        "def match_ramal(index, list_number):\n",
        "    # Access the bit at the given index for each number\n",
        "    bit_list = [ramal_dict[list_number[0]][index], ramal_dict[list_number[1]][index], ramal_dict[list_number[2]][index], ramal_dict[list_number[3]][index]]\n",
        "\n",
        "    return bit_list\n",
        "\n",
        "\n",
        "def get_ramal_number_from_ramal_dic(number, dic, count):\n",
        "\n",
        "    dic[count] = number\n",
        "\n",
        "\n",
        "# Function to mul numbers based on the three rules\n",
        "def multiply_number(num1, num2):\n",
        "    # Get the ramal values (bit representations) for both numbers\n",
        "    bits1 = ramal_dict[num1]\n",
        "    bits2 = ramal_dict[num2]\n",
        "\n",
        "    # Initialize a list to hold the new number's bits\n",
        "    new_number_bits = []\n",
        "\n",
        "    # Apply the rules for each index\n",
        "    for i in range(4):  # Assuming 4 bits\n",
        "        if bits1[i] == 0 and bits2[i] == 0:\n",
        "            new_number_bits.append(1)  # Rule 1\n",
        "        elif bits1[i] == 1 and bits2[i] == 1:\n",
        "            new_number_bits.append(1)  # Rule 2\n",
        "        else:\n",
        "            new_number_bits.append(0)  # Rule 3 (0 and 1 or 1 and 0)\n",
        "\n",
        "    # Return the new number in both bit format and cvalue format\n",
        "    return new_number_bits\n",
        "\n",
        "\n",
        "def search_and_add_to_dic(bit_list,dic):\n",
        "\n",
        "    key = find_number_with_bit_list(bit_list)\n",
        "    size = len(dic)\n",
        "    # Find the matching ramal value\n",
        "    dic[size] = key\n",
        "\n",
        "def display(dic):\n",
        "    print(\" \",dic[7],\"h#8 thinking\",\"     \", dic[6],\"h#7 wife\",\"     \", dic[5], \"h#6 destiny\",\"     \", dic[4], \"h#5 siblings\",\"     \", dic[3],\"h#4 personal house\",\"     \", dic[2],\"h#3 siblings\",\"     \", dic[1], \"h#2 business\",\"     \", dic[0], \"h#1 person/zat\")\n",
        "    print(\"                                                         |\")\n",
        "\n",
        "    print(\"          \", dic[11],\"h#12 enemy\",\"                        \", dic[10], \"h#11 friends\",\"                        \", dic[9],\"h#10 parents\",\"                        \", dic[8], \"h#9 travel\")\n",
        "    print(\"                                                         |\")\n",
        "\n",
        "    print(\"                            \", dic[13], \"                                                                  \", dic[12])\n",
        "    print(\"                                                         |\")\n",
        "\n",
        "    print(\"                            \", dic[14], \"                                                                  \", dic[15])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def interpret_ramal(dic):\n",
        "    interpretations = {\n",
        "        \"person/zat\": \"Represents self-identity, personality, and core traits.\",\n",
        "        \"business\": \"Indicates career, financial stability, and work-related matters.\",\n",
        "        \"siblings\": \"Represents family ties, support system, and communication.\",\n",
        "        \"personal house\": \"Reflects home life, well-being, and comfort.\",\n",
        "        \"destiny\": \"Indicates long-term fate and life path.\",\n",
        "        \"wife\": \"Represents relationships, partnerships, and marriage.\",\n",
        "        \"thinking\": \"Symbolizes mental state, thoughts, and worries.\",\n",
        "        \"enemy\": \"Represents obstacles, hidden opponents, or struggles.\",\n",
        "        \"friends\": \"Indicates allies, social circle, and external support.\",\n",
        "        \"parents\": \"Represents family influence, traditions, and elders.\",\n",
        "        \"travel\": \"Indicates movement, travel opportunities, and changes.\",\n",
        "        \"witness_1\": \"Summarizes past experiences and struggles.\",\n",
        "        \"witness_2\": \"Provides clarity on the current state of affairs.\",\n",
        "        \"judge\": \"Represents the final outcome and lifeâ€™s conclusion.\",\n",
        "        \"reconciler\": \"Optional: Hidden influence or unseen forces at play.\"\n",
        "    }\n",
        "\n",
        "    keys = [\"thinking\", \"wife\", \"destiny\", \"siblings\", \"personal house\", \"siblings\", \"business\", \"person/zat\",\n",
        "            \"enemy\", \"friends\", \"parents\", \"travel\", \"witness_1\", \"witness_2\", \"judge\", \"reconciler\"]\n",
        "\n",
        "    print(\"\\n--- Ramal Reading Interpretation ---\\n\")\n",
        "\n",
        "    for i, key in enumerate(keys):\n",
        "        if i in dic:\n",
        "            print(f\"{dic[i]} ({key}): {interpretations[key]}\")\n",
        "\n",
        "    print(\"\\nFinal Outcome:\")\n",
        "    print(f\"Judge ({dic[14]}): {interpretations['judge']}\")\n",
        "    print(f\"Reconciler ({dic[15]}): {interpretations['reconciler']}\")\n",
        "\n",
        "\n",
        "def main(list_number):\n",
        "    # Calculate ramal values first\n",
        "    calculate_ramal()\n",
        "    dic = {}\n",
        "\n",
        "\n",
        "    size = len(list_number)\n",
        "    for i in range(size):\n",
        "        get_ramal_number_from_ramal_dic(list_number[i], dic,i)\n",
        "\n",
        "    # this will create next 4 numbers and add to dic\n",
        "    for i in range(4):\n",
        "        bit_list = match_ramal(i, list_number)\n",
        "\n",
        "        search_and_add_to_dic(bit_list, dic)\n",
        "\n",
        "\n",
        "    # Iterate over the keys in steps of 2 to access pairs\n",
        "    for i in range(0, 14, 2):\n",
        "        keys1 = dic[i]\n",
        "        keys2 = dic[i+1]\n",
        "\n",
        "        bit_list = multiply_number(keys1, keys2)\n",
        "        search_and_add_to_dic(bit_list, dic)\n",
        "\n",
        "\n",
        "    bit_list = multiply_number(dic[14], dic[0])\n",
        "    search_and_add_to_dic(bit_list, dic)\n",
        "    # interpret_ramal(dic)\n",
        "    # display(dic)\n",
        "    for i in range(16):\n",
        "        # print(\"H# \",i+1,\": value: \",dic[i])\n",
        "\n",
        "        print(\"H# \",i+1,\": value: \",ramal_values[dic[i]], \"=>\", dic[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random_numbers = [random.randint(1, 16) for _ in range(4)]\n",
        "list_number = random_numbers\n",
        "# list_number = [14, 15, 4, 8]\n",
        "\n",
        "main(list_number)    #14 , 15 result in 1 in travel or 16 , 1 or 10, 11, or 13, 12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6URk6tIuUvZN",
        "outputId": "ba782df4-15e0-4356-9caa-48b89ca11a35"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H#  1 : value:  [1, 1, 1, 1] => 16\n",
            "H#  2 : value:  [0, 0, 1, 0] => 11\n",
            "H#  3 : value:  [1, 0, 1, 0] => 10\n",
            "H#  4 : value:  [0, 1, 0, 0] => 13\n",
            "H#  5 : value:  [1, 0, 1, 0] => 10\n",
            "H#  6 : value:  [1, 0, 0, 1] => 6\n",
            "H#  7 : value:  [1, 1, 1, 0] => 8\n",
            "H#  8 : value:  [1, 0, 0, 0] => 14\n",
            "H#  9 : value:  [0, 0, 1, 0] => 11\n",
            "H#  10 : value:  [0, 0, 0, 1] => 7\n",
            "H#  11 : value:  [1, 1, 0, 0] => 12\n",
            "H#  12 : value:  [1, 0, 0, 1] => 6\n",
            "H#  13 : value:  [1, 1, 0, 0] => 12\n",
            "H#  14 : value:  [1, 0, 1, 0] => 10\n",
            "H#  15 : value:  [1, 0, 0, 1] => 6\n",
            "H#  16 : value:  [1, 0, 0, 1] => 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 yes\n",
        "# 2 no"
      ],
      "metadata": {
        "id": "wdwKECfqdbbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.randint(1, 3)"
      ],
      "metadata": {
        "id": "9dHCVU9r4LT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q8X1rctxwrEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test"
      ],
      "metadata": {
        "id": "j1dhp5wowV9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = {\n",
        "      \"ref_13_1\": \"long short-term memory\",\n",
        "      \"ref_7_2\": \"gated recurrent\",\n",
        "      \"ref_35_3\": \"sequence modeling and transduction problems\",\n",
        "      \"ref_2_4\": \"competitive neural sequence transduction models\",\n",
        "      \"ref_7_5\": \"transduction\",\n",
        "      \"ref_7_6\": \"abc\",\n",
        "      \"ref_13_7\": \"cnn\",\n",
        "      \"ref_2_8\": \"competitive neural sequence transduction models\"\n",
        "\n",
        "    }"
      ],
      "metadata": {
        "id": "6vUVyhdq3bmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = {\n",
        "    \"ref_13_1\": \"long short-term memory\",\n",
        "    \"ref_7_2\": \"gated recurrent\",\n",
        "    \"ref_35_3\": \"sequence modeling and transduction problems\",\n",
        "    \"ref_2_4\": \"sequence modeling and transduction problems\",\n",
        "    \"ref_5_5\": \"sequence modeling and transduction problems\",\n",
        "    \"ref_38_6\": \"recurrent language models and encoder-decoder architectures\",\n",
        "    \"ref_24_7\": \"recurrent language models and encoder-decoder architectures\",\n",
        "    \"ref_15_8\": \"recurrent language models and encoder-decoder architectures\",\n",
        "    \"ref_21_9\": \"factorization tricks\",\n",
        "    \"ref_32_10\": \"conditional computation\",\n",
        "    \"ref_2_11\": \"dependencies without regard to their distance\",\n",
        "    \"ref_19_12\": \"dependencies without regard to their distance\",\n",
        "    \"ref_27_13\": \"used in conjunction with a recurrent network\",\n",
        "    \"ref_16_14\": \"Extended Neural GPU\",\n",
        "    \"ref_18_15\": \"ByteNet\",\n",
        "    \"ref_9_16\": \"ConvS2S\",\n",
        "    \"ref_12_17\": \"learn dependencies between distant positions\",\n",
        "    \"ref_4_18\": \"reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations\",\n",
        "    \"ref_27_19\": \"reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations\",\n",
        "    \"ref_28_20\": \"reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations\",\n",
        "    \"ref_22_21\": \"reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations\",\n",
        "    \"ref_34_22\": \"end-to-end memory networks\",\n",
        "    \"ref_38_23\": \"encoder-decoder attention mechanisms\",\n",
        "    \"ref_2_24\": \"encoder-decoder attention mechanisms\",\n",
        "    \"ref_9_25\": \"encoder-decoder attention mechanisms\",\n",
        "    \"ref_17_26\": \"models such as\",\n",
        "    \"ref_18_27\": \"models such as\",\n",
        "    \"ref_9_28\": \"models such as\",\n",
        "    \"ref_5_29\": \"competitive neural sequence transduction models\",\n",
        "    \"ref_2_30\": \"competitive neural sequence transduction models\",\n",
        "    \"ref_35_31\": \"competitive neural sequence transduction models\",\n",
        "    \"ref_10_32\": \"auto-regressive\",\n",
        "    \"ref_11_33\": \"residual connection\",\n",
        "    \"ref_1_34\": \"layer normalization\",\n",
        "    \"ref_30_35\": \"learned linear transformation and softmax function\",\n",
        "    \"ref_31_36\": \"byte-pair representations\",\n",
        "    \"ref_3_37\": \"byte-pair encoding\",\n",
        "    \"ref_33_38\": \"dropout\",\n",
        "    \"ref_36_39\": \"label smoothing\",\n",
        "    \"ref_20_40\": \"Adam optimizer\",\n",
        "    \"ref_38_41\": \"beam search\",\n",
        "    \"ref_39_42\": \"Deep-Att + PosUnk\",\n",
        "    \"ref_18_43\": \"ByteNet\",\n",
        "    \"ref_38_44\": \"GNMT + RL\",\n",
        "    \"ref_9_45\": \"ConvS2S\",\n",
        "    \"ref_32_46\": \"MoE\",\n",
        "    \"ref_39_47\": \"Deep-Att + PosUnk Ensemble\",\n",
        "    \"ref_38_48\": \"GNMT + RL Ensemble\",\n",
        "    \"ref_9_49\": \"ConvS2S Ensemble\",\n",
        "    \"ref_25_50\": \"Penn Treebank\",\n",
        "    \"ref_37_51\": \"RNN sequence-to-sequence models\",\n",
        "    \"ref_37_52\": \"BerkeleyParser semi-supervised corpora\",\n",
        "    \"ref_37_53\": \"RNN sequence-to-sequence models\",\n",
        "    \"ref_37_54\": \"Vinyals & Kaiser el al.\",\n",
        "    \"ref_37_55\": \"Vinyals & Kaiser el al.\",\n",
        "    \"ref_29_56\": \"BerkeleyParser\",\n",
        "    \"ref_29_57\": \"Petrov et al.\",\n",
        "    \"ref_40_58\": \"Zhu et al.\",\n",
        "    \"ref_8_59\": \"Dyer et al.\",\n",
        "    \"ref_14_60\": \"Huang & Harper\",\n",
        "    \"ref_26_61\": \"McClosky et al.\",\n",
        "    \"ref_23_62\": \"Luong et al.\",\n",
        "    \"ref_38_63\": \"Google's Neural Machine Translation System\",\n",
        "    \"ref_38_64\": \"Word-piece representations\",\n",
        "    \"ref_38_65\": \"Word-piece vocabulary\",\n",
        "    \"ref_38_66\": \"Beam search with length penalty\",\n",
        "    \"ref_38_67\": \"Early stopping during inference\",\n",
        "    \"ref_38_68\": \"GNMT + RL\",\n",
        "    \"ref_38_69\": \"GNMT + RL Ensemble\",\n",
        "    \"ref_39_70\": \"Deep-Att + PosUnk\",\n",
        "    \"ref_39_71\": \"Deep-Att + PosUnk Ensemble\",\n",
        "    \"ref_40_72\": \"Zhu et al.\",\n",
        "    \"ref_40_73\": \"Zhu et al.\"\n",
        "}"
      ],
      "metadata": {
        "id": "ZmC5rZcJFOpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = {\n",
        "    \"ref_Smith2020_1\": \"This method was validated through large-scale trials (Smith, 2020).\",\n",
        "    \"ref_Luo|Bhattacharya2006_2\":  \"Corporate reputation reflects stakeholders perceptions (Luo and Bhattacharya, 2006).\",\n",
        "    \"ref_Hameed2021_3\":  \"CR as a collective perception of the past activities and beliefs of the firm regarding its future activities (Rettab and Mellahi, 2019; Hameed et al., 2021).\",\n",
        "    \"ref_Rettab|Mellahi2019_4\":  \"CR as a collective perception of the past activities and beliefs of the firm regarding its future activities (Rettab and Mellahi, 2019; Hameed et al., 2021).\",\n",
        "    \"ref_Smith2020_5\": \"Newburry et al. (2019) define CR as a collective perception of the past activities and beliefs of the firm regarding its future activities\",\n",
        "    \"ref_Hameed2021_6\": \"Newburry et al. (2019) define CR as a collective perception of the past activities and beliefs of the firm regarding its future activities\"\n",
        "}"
      ],
      "metadata": {
        "id": "_z1zdyBOn1o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b ={}"
      ],
      "metadata": {
        "id": "CYIY_bKgZWst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in a:\n",
        "    ref = i.split(\"_\")\n",
        "    if \"ref_\"+str(ref[1]) in b:\n",
        "        size = len(b[\"ref_\"+str(ref[1])])\n",
        "        b[\"ref_\"+str(ref[1])][str(size+1)] = a[i]\n",
        "    else:\n",
        "        b[\"ref_\"+str(ref[1])] ={}\n",
        "        b[\"ref_\"+str(ref[1])][\"1\"] = a[i]\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qXtnlYfqY1F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "id": "IOZetfTRbS6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = {\n",
        "\t\"ref_1\": {\n",
        "        \"1\": \"layer normalization\"\n",
        "    },\n",
        "\t\"ref_2\": {\n",
        "        \"1\": \"language modeling and machine translation\",\n",
        "        \"2\": \"Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences\",\n",
        "        \"3\": \"neural sequence transduction models have an encoder-decoder structure\",\n",
        "        \"4\": \"additive attention\",\n",
        "        \"5\": \"Neural machine translation by jointly learning to align and translate\"\n",
        "    },\n",
        "\t\"ref_3\": {\n",
        "        \"1\": \"additive attention outperforms dot product attention without scaling for larger values of dk\",\n",
        "        \"2\": \"byte-pair encoding\"\n",
        "    },\n",
        "\t\"ref_4\": {\n",
        "        \"1\": \"Self-attention used successfully in tasks including reading comprehension, abstractive summarization, textual entailment and sentence representations\"\n",
        "    },\n",
        "\t\"ref_5\": {\n",
        "        \"1\": \"language modeling and machine translation\",\n",
        "        \"2\": \"neural sequence transduction models have an encoder-decoder structure\"\n",
        "    },\n",
        "\t\"ref_6\": {\n",
        "        \"1\": \"Separable convolutions\"\n",
        "    },\n",
        "\t\"ref_7\": {\n",
        "        \"1\": \"gated recurrent neural networks\"\n",
        "    },\n",
        "\t\"ref_8\": {\n",
        "        \"1\": \"Recurrent Neural Network Grammar\",\n",
        "        \"2\": \"Dyer et al. (2016)\"\n",
        "    },\n",
        "\t\"ref_9\": {\n",
        "        \"1\": \"ConvS2S\",\n",
        "        \"2\": \"Convolutional sequence to sequence learning\",\n",
        "        \"3\": \"positional encodings, learned and fixed\",\n",
        "        \"4\": \"learned positional embeddings\",\n",
        "        \"5\": \"learned positional embeddings\",\n",
        "        \"6\": \"ConvS2S Ensemble\"\n",
        "    },\n",
        "\t\"ref_10\": {\n",
        "        \"1\": \"model is auto-regressive, consuming previously generated symbols\"\n",
        "    },\n",
        "\t\"ref_11\": {\n",
        "        \"1\": \"residual connection\"\n",
        "    },\n",
        "\t\"ref_12\": {\n",
        "        \"1\": \"Difficulty learning dependencies between distant positions\",\n",
        "        \"2\": \"The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies\"\n",
        "    },\n",
        "\t\"ref_13\": {\n",
        "        \"1\": \"long short-term memory\"\n",
        "    },\n",
        "\t\"ref_14\": {\n",
        "        \"1\": \"Huang & Harper (2009)\"\n",
        "    },\n",
        "\t\"ref_15\": {\n",
        "        \"1\": \"recurrent language models and encoder-decoder architectures\"\n",
        "    },\n",
        "\t\"ref_16\": {\n",
        "        \"1\": \"Extended Neural GPU\"\n",
        "    },\n",
        "\t\"ref_17\": {\n",
        "        \"1\": \"Neural GPU\"\n",
        "    },\n",
        "\t\"ref_18\": {\n",
        "        \"1\": \"ByteNet\",\n",
        "        \"2\": \"dilated convolutions\"\n",
        "    },\n",
        "\t\"ref_19\": {\n",
        "        \"1\": \"Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences\"\n",
        "    },\n",
        "\t\"ref_20\": {\n",
        "        \"1\": \"Adam optimizer\"\n",
        "    },\n",
        "\t\"ref_21\": {\n",
        "        \"1\": \"factorization tricks\"\n",
        "    },\n",
        "\t\"ref_22\": {\n",
        "        \"1\": \"Self-attention used successfully in tasks including reading comprehension, abstractive summarization, textual entailment and sentence representations\"\n",
        "    },\n",
        "\t\"ref_23\": {\n",
        "        \"1\": \"Luong et al. (2015)\"\n",
        "    },\n",
        "\t\"ref_24\": {\n",
        "        \"1\": \"recurrent language models and encoder-decoder architectures\"\n",
        "    },\n",
        "\t\"ref_25\": {\n",
        "        \"1\": \"Penn Treebank (WSJ portion)\"\n",
        "    },\n",
        "\t\"ref_26\": {\n",
        "        \"1\": \"McClosky et al. (2006)\"\n",
        "    },\n",
        "\t\"ref_27\": {\n",
        "        \"1\": \"attention mechanisms are used in conjunction with a recurrent network\",\n",
        "        \"2\": \"Self-attention used successfully in tasks including reading comprehension, abstractive summarization, textual entailment and sentence representations\"\n",
        "    },\n",
        "\t\"ref_28\": {\n",
        "        \"1\": \"Self-attention used successfully in tasks including reading comprehension, abstractive summarization, textual entailment and sentence representations\"\n",
        "    },\n",
        "\t\"ref_29\": {\n",
        "        \"1\": \"Berkeley Parser\",\n",
        "        \"2\": \"Petrov et al. (2006)\"\n",
        "    },\n",
        "\t\"ref_30\": {\n",
        "        \"1\": \"weight sharing between embedding layers and softmax\"\n",
        "    },\n",
        "\t\"ref_31\": {\n",
        "        \"1\": \"byte-pair representations\"\n",
        "    },\n",
        "\t\"ref_32\": {\n",
        "        \"1\": \"conditional computation\",\n",
        "        \"2\": \"MoE\"\n",
        "    },\n",
        "\t\"ref_33\": {\n",
        "        \"1\": \"dropout\"\n",
        "    },\n",
        "\t\"ref_34\": {\n",
        "        \"1\": \"End-to-end memory networks are based on a recurrent attention mechanism have been shown to perform well on simple-language question answering and language modeling tasks\"\n",
        "    },\n",
        "\t\"ref_35\": {\n",
        "        \"1\": \"language modeling and machine translation\",\n",
        "        \"2\": \"neural sequence transduction models have an encoder-decoder structure\"\n",
        "    },\n",
        "\t\"ref_36\": {\n",
        "        \"1\": \"Label Smoothing\"\n",
        "    },\n",
        "\t\"ref_37\": {\n",
        "        \"1\": \"RNN sequence-to-sequence models\",\n",
        "        \"2\": \"BerkeleyParser semi-supervised corpora\",\n",
        "        \"3\": \"RNN sequence-to-sequence models\",\n",
        "        \"4\": \"Vinyals & Kaiser el al. (2014)\",\n",
        "        \"5\": \"Vinyals & Kaiser el al. (2014)\"\n",
        "    },\n",
        "\t\"ref_38\": {\n",
        "        \"1\": \"recurrent language models and encoder-decoder architectures\",\n",
        "        \"2\": \"Google's Neural Machine Translation System\",\n",
        "        \"3\": \"Word-piece representations\",\n",
        "        \"4\": \"Word-piece vocabulary\",\n",
        "        \"5\": \"Beam search with length penalty\",\n",
        "        \"6\": \"Early stopping during inference\",\n",
        "        \"7\": \"GNMT + RL\",\n",
        "        \"8\": \"GNMT + RL Ensemble\"\n",
        "    },\n",
        "\t\"ref_39\": {\n",
        "        \"1\": \"Deep-Att + PosUnk\",\n",
        "        \"2\": \"Deep-Att + PosUnk Ensemble\"\n",
        "    },\n",
        "\t\"ref_40\": {\n",
        "        \"1\": \"Zhu et al. (2013)\",\n",
        "        \"2\": \"Zhu et al. (2013)\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "Hi03uyl4d0vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a= {}\n",
        "counter =1\n",
        "\n",
        "for i in b:\n",
        "    ref = i.split(\"_\")\n",
        "    size = len(b[i])\n",
        "    for j in range(1,size+1):\n",
        "        a[\"ref_\"+str(ref[1])+\"_\"+str(counter)] = b[i][str(j)]\n",
        "        counter+=1"
      ],
      "metadata": {
        "id": "CvR0BQtMzsyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "id": "m_qMNF8xz5bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZRdDZ0_u1NeK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}